---
layout:     post
title:      "Entropy and Information [IT.1]"
subtitle:   "The relationship between entropy and information"
date:       2017-12-20 12:00:00
author:     "peyrardm"
header-img: "img/bg_home.jpg"
---
				
<p>Information theory is the field that quantifies rigorously the notion of information, which is highly relevant to understand patterns emerging from natural and mathematical phenomena. Hence, it has found wide-ranging applications in physics {% cite Jaynes_1957  -A %}, economics {% cite EcoIT -A %}, biology {% cite Adami_2012 -A %},  machine learning {% cite tishby99 -A %} and even consciousness {% cite IIT -A %}.</p>

<p>Information theory primarily intends to measure information and track its variations via a mathematical object called Entropy. Entropy was introduced by Shannon {% cite Shannon_1963 -A %}	 as a measure of uncertainty and information, but the term was borrowed from thermodynamics. The resulting notions from communication theory and physics can be united but render different views on the notion of information. As a result, understanding how information and entropy relate to each other can be unclear. </p>

<h2>Entropy measures uncertainty</h2>
<hr>					
<p>Imagine a system which can be in one state among \(N\) possible states. We don't know precisely its future evolution, but our assumptions about the next state are described by a random variable \(X\) following a probability distribution: \(P(X=x_i) = p_i\). Shannon's <em>entropy</em> \(H\) is a measure of our uncertainty about the next state:
$$
	H(X) = - \sum\limits_{x_i \in \Omega} p_i \cdot \log(p_i)
$$
</p>

<p> \(H(X)\) is maximal when the probability distribution is uniform ( i.e. all states are equally likely). Also, it is \(0\) when there exists one state \(x_i\) such that \(p_i = 1\), when we know the next state of \(X\) with no ambiguity. In between, \(H(X)\) quantifies this uncertainty.
</p>

<p>Observe that \(-\log(p_i)\) is interpreted as the surprisal of observing the state \(x_i\) given our assumptions. Indeed, if \(p_i\) is small, \(-\log(p_i)\) is large and we would be surprised to observe \(x_i\).  Therefore, entropy is the average surprise. </p>

<p>If we can query an oracle about the next state of \(X\), but only with "Yes/No" questions. With an optimal strategy, what is the average number of queries we have to make before knowing the answer unambiguously? It is the entropy (up to some rounding because entropy is continuous): the minimum amount of "Yes/No" questions (or bits) necessary to describe this outcome. For this reason, we say that entropy is the average amount of information we need to transmit in order to convey the outcome. Intuitively, we don't need much information to communicate a probable outcome, while a surprising result requires more information.</p>

<p>Entropy is a measure of uncertainty, average surprisal and, according to Shannon, a measure of information.</p>
<!-- <p>Note: When we say we do not know anything about \(X\) we still somehow know what are the possible state. Doesn't this count as information about \(X\)? It is important to note that the question we are asking is not about \(X\) itself but about the behaviour of \(X\). The question is about which state will it take not about what \(X\) is. The uncertainty we are trying to quantify is about \(X\)'s behaviour, but we could also ask a different question: how many states exists for \(X\)? This would be subject to its own probability distribution over \(\mathbb{N}\) and subject to its own uncertainty measure. </p> -->


<!-- <p>Entropy, uncertainty and ultimately information depend on the observer for two reasons: (1) Which question do we ask? The uncertainty can only be measured as the uncertainty over the answer to some specific question. (2) What we already know. There is a probability distribution over the possible answer to our question to measure the uncertainty. As we said before, the probability distribution is uniform when we know nothing about the answer but as soon as we have some <em>information</em> the distribution changes. </p> -->

<h2>Information</h2>
<hr>
<p>With the same system \(X\), we can have a different intuitive view on information based on the following considerations:</p>

<p>(1) If we possess no <b>information</b> about the behavior of the system, then the answer to the question "what is the next state?" is a uniform distribution over states. No information corresponds to maximum uncertainty (entropy).</p>

<p>(2) Should we gain information about the system, we would recognize some states as more likely than others. Mathematically, we would have a new distribution over states different from the uniform. In fact, any non-uniform probability distribution contains <b>information</b> because it says which outcomes are more likely. <b>Information</b> is ultimately a measure of our capacity of making predictions (better than random) and non-uniform means better predictions than pure random guess.</p>

<!-- <p>(3) When we know everything there is to know about the system (we gained all information possible), then we can predict exactly the state in which the system will be. We have no more uncertainty. The probability distribution is \(1\) for the one state which will definitely be the outcome and \(0\) for all the others. This corresponds to no uncertainty and an entropy of \(0\).</p> -->

<p>Now we are thinking about gain or loss of information, which are quantified by the fluctuations of entropy (uncertainty). In absence of information, we assume a uniform distribution. Gaining information moves the distribution away from the uniform, causing a decrease in entropy \(H\). Consequently, the quantity of <b>information</b> gained is precisely this change in entropy:
$$
	I(X) = H(U) - H(X)
$$
If we didn't know anything, we would have to make \(H(U)\) queries, but only \(H(X)\) suffice: we gained the equivalent of \(H(U) - H(X)\) bits of information by knowing the distribution of \(X\). Thus, \(I(X)\) measures the amount of information we possess, and \(H(X)\) the amount of information we still need to obtain before knowing \(X\) exactly.
</p>

<p>In this Bayesian viewpoint, we necessarily begin with a uniform distribution, which changes with the acquisition of knowledge. 
In fact, every non-uniform probability distribution is conditional because whenever the probability distribution deviates from the uniform, it contains information which comes from some knowledge source. One natural interpretation is to view probability distribution as the answer to the question "what will be the outcome?" given the knowledge we currently have. \(I\) quantifies this knowledge in bits.</p>

<!-- <h3>Mutual Information</h3>

<p>First, observe that every non-uniform probability distribution is conditional. Indeed, whenever the probability distribution deviates from the uniform distribution, it is because we have some knowledge and we are conditioning on this new knowledge. The probability distribution is the answer to the question "what will be the outcome?" given the knowledge we have.</p>

<p>Now, suppose we have two random variables (systems) \(X\) and \(Y\) each following a probability distribution. Now, what happens if we gain knowledge about \(Y\), do we also gain information about \(X\)? If \(X|Y\) is the same as \(X\) then observing \(Y\) gave us no information about \(X\) and we say that \(X\) and \(Y\) are independent.</p>

<p>If we gained information about \(X\) by observing \(Y\), the uncertainty (entropy) of \(X\) should decrease (move away from the uniform distribution): \(H(X|Y) \le H(X)\). If observing \(Y\) reduced the uncertainty we have about \(X\) and we can quantify this reduction: 
	$$
		I(X|Y) = H(X) - H(X|Y)
	$$
There are two non-trivial facts to observe here: (1) \(I(X|Y) = I(Y|X)\) therefore we note it \(I(X,Y)\). It means that the amount of information we gain about \(X\) by observing \(Y\) is always the same as the amount of information we gain about \(Y\) by observing \(X\). (2) \(I(X,Y) \geq 0\), meaning that \(H(X|Y) \leq H(X)\) (with equality when \(X\) and \(Y\) are independent). It has the intuitive explanation that observing \(Y\) (learning something) can not increase the uncertainty about \(X\), at worst it doesn't tell us anything new about \(X\).
</p>

<p>Now, if \(X\) is non-uniform it is conditional; there exists a source of knowledge \(K\) such that \(X = U|K\). We can observe that what we defined to be \(I(X)\) takes the shape of a mutual information between the uniform distribution and this \(K\):
$$
	I(X) = H(U) - H(X) = H(U) - H(U|K) =  I(K, U) 
$$
In the very beginning, we start with the uniform distribution \(U\) and gain some information. This information induces a new probability distribution \(X\) (=\(U|K\)). The quantity of information we gained is measured by \(I(X) = I(K, U)\). Later, we may observe \(Y\) which updates the probability distribution to a new one \(X|Y\) with lower uncertainty, and so on until we know everything or cannot gain more information.
</p> -->


<h2>Potential Information</h2>
<p>According the previous insights, it appears that <em>entropy</em> and <em>information</em> move in reverse direction. Gaining information reduces entropy (uncertainty) and high entropy indicates a lack of information: information is the difference between the maximum entropy and the actual entropy. Yet, we also remember that Shannon originally introduced entropy directly as a measure of information. There seems to be a contradiction here.
	<!-- In particular, <em>self-information</em> or <em>information content</em> induced by one state is given by \(\log(\frac{1}{p_i})\). These two notion of <em>information</em> seem contradictory. -->
</p>

<p>In fact, the new notion information is more akin to physical views of Von Neumann, where <em>entropy</em> may be viewed as a physical measure of the lack of physical information about the details of a system. These discrepancies arise because the two fields are interested in two different questions. </p>

<p>The physical view intends to measure our ability to predict the evolution of the system (done by \(I(X)\)). Communication theory desires to measure how much it should transmit to describe the next state (done by \(H(X)\)). The two perceptions are basically <em>equivalent</em> because they are tightly connected via the constant maximum entropy \(H(U)\): \(I(X) = H(U) - H(X)\). </p>

<h3>Example</h3>
<p>To further this point, we can study an example. Let \(Y\) and \(Z\) be two random variables generating two texts: \(Y\) generates a boring text with one letter repeated \(n\) times while \(Z\) is proper English. </p>

<p>Intuitively, we'd say that \(Y\) does not contain much information. Our intuition follows Shannon's notion: the text contains little information because we can communicate its content easily. The other notion says: we can predict it perfectly, therefore, we possess all the information.</p>

<p>According to Shannon, \(Z\) contains a lot of information because we need many "Yes/No" questions to transmit its content. The physics view states: there must be information missing before we can predict \(Z\) perfectly. Intuitively, the more surprising a text is the more information we could learn about the process that generated it.</p>

<!-- <p>Remark that this assumes a deterministic world: we can learn about \(X\) until there is no randomness and perfect predictions. Imagine there exists an object with true randomness. We can obtain all the information but its entropy never decreases below a minimal \(H \ge 0\). Then this minimal entropy is the measure of its randomness, of its intrinsic incompressibility. Incompressibility because it is the amount of information inside this object which cannot come from elsewhere (other source of knowledge). (The question of whether true randomness exists is out of scope, but in practice we observe randomness.)</p> -->

<h3>Consequences</h3>
<p>The safe view is to interpret entropy as a measure of uncertainty. Entropy is also a measure of information but whether entropy and information evolve in the same direction or not depends on what we question we ask: what do we already know? or what remains to be known?</p>

<p>In general, we can assume that entropy represents the <b>potential information</b>: the amount of information we have to gain before knowing the outcome exactly. Information in Shannon's view is this potential information.</p> 

<p>Note: Identifying the "entropy as a lack of information" intuition to the physics idea of information is a bit hasty. Here, it seems that entropy and information depend on an observer and its assumptions. However, in physics, entropy is defined more subtly: it is independent of any observer since it captures a statistical property of a system.</p>

# References
{% bibliography --cited %}
<!-- <p>Finally, we see that the paradox is resolved by understanding what we mean when we use the word <em>information</em>. Therefore, a text is more interesting if it is more complex because it exhibits more potential information. However, when measuring our uncertainty about how a system works, our information corresponds to how accurately we can predict its behaviour. In other words, information I(X) answers the question: what do we know about \(X\)? and entropy (or potential information) anwers the question: how much more is there to know about \(X\)? or how much more can I hope to learn by observing more of \(X\)?</p> -->